{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "name": "ICP.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ytpOz863N0Cw"
      },
      "source": [
        "Iterative Closest Point\n",
        "--------------------------------------\n",
        "\n",
        "This project implements an ICP algorithm and use it alongside GTSAM to perform simultaneous localization & mapping (SLAM) on Lidar scans."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UWTsuOS0poFv"
      },
      "source": [
        "Load in the datset. This cell finds filenames for all of the point cloud files (.ply) in the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VLFNdfUNIt8D"
      },
      "source": [
        "import os\n",
        "import math\n",
        "import gtsam\n",
        "import numpy as np\n",
        "from tqdm import trange\n",
        "import matplotlib.pyplot as plt\n",
        "import gtsam.utils.plot as gtsam_plot\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "\n",
        "from project_assets.helpers import *\n",
        "\n",
        "LIDAR_FPATH = \"project_assets/lidar/\"\n",
        "\n",
        "scans_fnames = []\n",
        "for file in sorted(os.listdir(LIDAR_FPATH)):\n",
        "    scans_fnames.append(os.path.join(LIDAR_FPATH, file))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KY7-vYBk03i9"
      },
      "source": [
        "# Visualization\n",
        "\n",
        "In this section, we'll become aquainted with the dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BQmor4-PUE-X"
      },
      "source": [
        "This dataset is composed of 180 Lidar scans (.ply files) captured by Argo AI, a self-driving car company based in Pittsburgh, PA. These scans were captured over 18 seconds by one of their cars in Miami, which was likely equipped with a Lidar sensor similar to this [one](https://velodynelidar.com/products/hdl-64e/). This 30 second video clip from the car's front camera gives a good intuition of what happens: https://youtu.be/FUDRK_0iEKA. The first 12 seconds, where the car is stationary, are not part of our dataset.\n",
        "\n",
        "It's night-time and the car starts at an offset T-intersection. It waits for a car, three bicyclists, and another car to pass (12 seconds that are not part of our dataset). Then it makes a left turn onto NW 2nd Ave and traveling down the street (18 seconds that is our dataset).\n",
        "\n",
        "In this cell, we read in the first Lidar scan and visualizes it at full resolution (~88,000 points). Use the plot menu to zoom, pan, and rotate around the scene. Then, change the index on `scans_fnames` to see some of the later frames."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bidPy9VgWBC-"
      },
      "source": [
        "visualize_clouds(read_ply(scans_fnames[0]), show_grid_lines=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NTFaNRqziiSI"
      },
      "source": [
        "This cell reads in the first 20 Lidar scans and visualizes them (as an animation) at a reduced resolution. Due to browser limitations, any additional frames after the first 5 must be rendered at reduced resolutions. Play around with the number of frames loaded by modifying the index on `scans_fnames`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k47ccR13nBEQ"
      },
      "source": [
        "# the more frames visualized, the lower the resolution of each cloud\n",
        "clouds_subset = read_ply(*scans_fnames[:20], as_animation=True)\n",
        "\n",
        "visualize_clouds_animation(clouds_subset, show_grid_lines=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K_nFAm9Y4yF6",
        "scrolled": false
      },
      "source": [
        "# visualizing the entire sequence at a low resolution\n",
        "clouds_all = read_ply(*scans_fnames, as_animation=True)\n",
        "\n",
        "visualize_clouds_animation(clouds_all)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jDV7-y-fLYGi"
      },
      "source": [
        "# Iterative Closest Point\n",
        "\n",
        "In this section, we'll load in two point clouds and implement the five steps of ICP to align them.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WuuRxjEt-obi"
      },
      "source": [
        "# we'll read in the first and eleventh clouds\n",
        "clouda = read_ply(scans_fnames[0])[0]\n",
        "cloudb = read_ply(scans_fnames[10])[0]\n",
        "print(clouda.shape, cloudb.shape)\n",
        "\n",
        "visualize_clouds([clouda, cloudb], show_grid_lines=True, cloud_colors=['#008B8B', '#FFA07A'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q6_XDj9vlblf"
      },
      "source": [
        "Let's take a moment to talk about the shape of these clouds. Each cloud is a numpy array of shape (3, n), where n is the number of point in the cloud. The first dimension is 3 because each point has an x, y, and z component. Therefore, cloud[0] would be an array of length n that contains the x component of each point in the cloud."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OCeaDwRGxdlV"
      },
      "source": [
        "There are five steps to ICP:\n",
        "1.   Initial transformation\n",
        "2.   Transform cloud\n",
        "3.   Assign closest point pairs\n",
        "4.   Estimate transformation\n",
        "5.   Repeat steps 2-4 for maximum number of iterations or change is very small\n",
        "\n",
        "Let's begin with Step 1: the initial tranformation\n",
        "\n",
        "The initial transform will be an input to the ICP algorithm. Below, we describe some methods through which the initial transform is found.\n",
        "\n",
        "As you can see in the visualization of the two clouds, the two clouds are nearly identical. This makes sense since the cloudb was captured 1 second after clouda. By hovering over the landmarks we expect to stay stationary on the street (such as parked cars along the street), we can see that cloudb is rotated and translated some x amount from clouda. For someone running icp on two clouds, they could use this guess-timate. Here's how this is done at a self-driving company. Tracking landmarks to compose the initial guess is common. Argo AI can (and does) write CNNs to detect and track landmarks (i.e. street signs, buildings, parked cars), and then computes the distance between stationary landmarks.\n",
        "\n",
        "Another method could be using kinematic information of the car. The car know's its heading, velocity, and acceleration, allowing us to use the elapsed time to estimate our transform.\n",
        "\n",
        "Even another method could be centroid of the point clouds. By computing translation between centroida and centroidb, we have half of a guess at the initial transform.\n",
        "\n",
        "Ultimately, the cloud pairs that are being ICP-ed together are captured less than a second apart. It is possible then to just use the identity transform as the initial guess."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "luA3gl2h68iG"
      },
      "source": [
        "## ICP helper functions\n",
        "\n",
        "When writing software, 'test driven development' refers to writing your expectation for the software as a unittest before writing the software itself. The pattern of development for this section should be:\n",
        "\n",
        "1. Review the docstring for the method\n",
        "2. Write some unit tests to set the expectation for what a correct implementation would look like\n",
        "3. Write the method itself\n",
        "\n",
        "Below are some basic unit tests."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0x0B94YA0J4k"
      },
      "source": [
        "import unittest\n",
        "\n",
        "class TestICPHelpers(unittest.TestCase):\n",
        "\n",
        "    def setUp(self):\n",
        "        self.testclouda = np.array([[1], [1], [1]])\n",
        "        self.testcloudb = np.array([[2, 10], [1, 1], [1, 1]])\n",
        "        self.testcloudc = np.array([[2], [1], [1]])\n",
        "        self.testbTa = gtsam.Pose3(gtsam.Rot3(), gtsam.Point3(1, 0, 0))\n",
        "        self.testcloudd = np.array([[0, 20, 10], [0, 10, 20], [0, 0, 0]])\n",
        "        self.testcloude = np.array([[10, 30, 20], [10, 20, 30], [0, 0, 0]])\n",
        "\n",
        "    def test_transform_cloud1(self):\n",
        "        expected = 2\n",
        "        actual = transform_cloud(self.testbTa, self.testclouda)[0][0]\n",
        "        self.assertEqual(expected, actual)\n",
        "\n",
        "    def test_assign_closest_pairs1(self):\n",
        "        expected = (3, 1)\n",
        "        actual = assign_closest_pairs(self.testclouda, self.testcloudb).shape\n",
        "        self.assertEqual(expected, actual)\n",
        "\n",
        "    def test_assign_closest_pairs2(self):\n",
        "        expected = 2\n",
        "        actual = assign_closest_pairs(self.testclouda, self.testcloudb)[0][0]\n",
        "        self.assertEqual(expected, actual)\n",
        "\n",
        "    def test_estimate_transform1(self):\n",
        "        expected = 1\n",
        "        actual = estimate_transform(self.testclouda, self.testcloudc).x()\n",
        "        self.assertEqual(expected, actual)\n",
        "\n",
        "    def test_estimate_transform2(self):\n",
        "        expected = 10\n",
        "        actual = estimate_transform(self.testcloudd, self.testcloude).x()\n",
        "        self.assertAlmostEqual(expected, actual, places=7)\n",
        "        actua2 = estimate_transform(self.testcloudd, self.testcloude).y()\n",
        "        self.assertAlmostEqual(expected, actua2, places=7)\n",
        "\n",
        "    def test_icp1(self):\n",
        "        ret = icp(self.testclouda, self.testcloudb)\n",
        "        expected1 = type(gtsam.Pose3())\n",
        "        actual1 = type(ret[0])\n",
        "        self.assertEqual(expected1, actual1)\n",
        "\n",
        "    def test_icp2(self):\n",
        "        expected = 1\n",
        "        actual = icp(self.testclouda, self.testcloudb)[0].x()\n",
        "        self.assertEqual(expected, actual)\n",
        "\n",
        "    def test_icp3(self):\n",
        "        expected = 1\n",
        "        actual = icp(self.testclouda, self.testcloudc)[0].x()\n",
        "        self.assertEqual(expected, actual)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "233YHBkB0J4k"
      },
      "source": [
        "Step 2: Transform cloud\n",
        "\n",
        "Given an input cloud, apply the gtsam.Pose3 transform on each point in the cloud. The recommended approach is convert each point in the point cloud into homogeneous points and apply a homogeneous transformation to each point."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lCvWLZZr1JTY"
      },
      "source": [
        "def transform_cloud(transform, clouda):\n",
        "    \"\"\"Transforms each point in a cloud\n",
        "    given a gtsam.Pose3 transform.\n",
        "    \"\"\"\n",
        "    transformed_cloud = clouda.copy() \n",
        "    transformed_cloud = np.dot(transform.matrix(), np.vstack((transformed_cloud, np.ones((1, transformed_cloud.shape[-1])))))\n",
        "    return transformed_cloud[:3,]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_pBR5pLt0J4l"
      },
      "source": [
        "suite = unittest.TestSuite()\n",
        "suite.addTest(TestICPHelpers('test_transform_cloud1'))\n",
        "unittest.TextTestRunner().run(suite)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UshWg4peMcYO"
      },
      "source": [
        "\n",
        "Step 3: Assign closest point pairs\n",
        "\n",
        "For each point in clouda, find the closest (euclidean-wise) point in cloudb and form a pair. Return a cloud shaped like clouda that has been rearranged to form closest pairs index-wise between points in clouda and points in rearranged cloudb."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QwCUoO9gMoIK"
      },
      "source": [
        "def assign_closest_pairs(clouda, cloudb):\n",
        "    \"\"\"Returns a a point cloud (cloudc) with points from cloudb, such that the\n",
        "    ith point in cloudc is cloudb's closest point to the ith point in clouda\n",
        "    by euclidean distance.\n",
        "    \"\"\"\n",
        "    transposeA = clouda.T\n",
        "    transposeB = cloudb.T\n",
        "    neigh = NearestNeighbors(n_neighbors=1, algorithm='kd_tree').fit(X=transposeB)\n",
        "    i = neigh.kneighbors(transposeA)[1]\n",
        "    cloudc = transposeB[i[:,0]].T #i[0,:]\n",
        "    return cloudc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RIyDauGY0J4m"
      },
      "source": [
        "suite = unittest.TestSuite()\n",
        "suite.addTest(TestICPHelpers('test_assign_closest_pairs1'))\n",
        "suite.addTest(TestICPHelpers('test_assign_closest_pairs2'))\n",
        "unittest.TextTestRunner().run(suite)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DkVK2YOqd2IK"
      },
      "source": [
        "Estimate transform (This step is given by the TAs.)\n",
        "\n",
        "In normal matrix multiplication, we apply a transformation matrix $A$ to a given input vector $x$ to give us an output vector $b$ (i.e., $Ax=b$). Our output here represents a transformation matrix such that our input vector `clouda` transforms into output vector `cloudb` (i.e, $A=bx^{-1}$).\n",
        "\n",
        "Thus, given clouda and cloudb, this function will return a `gtsam.pose3` object (consisting of a rotational and translational component) that we can use to estimate a transform that aligns clouda with cloudb (i.e., minimizing the point-to point-distances from clouda to cloudb).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D7l-Se95arwP"
      },
      "source": [
        "def estimate_transform(clouda, cloudb):\n",
        "    \"\"\"Estimate the transform from clouda to cloudb that minimize the sum-of-square\n",
        "    of the distances from the transformed points in point cloud A to the corresponding\n",
        "    points in point cloud B.\n",
        "        e.g.  argmin Σ||T*a_i - b_i||^2\n",
        "                T    i\n",
        "    \"\"\"\n",
        "    if clouda.shape != cloudb.shape and clouda.shape[1] < 3:\n",
        "        return None\n",
        "    \n",
        "    centroida = np.average(clouda, axis=1)\n",
        "    centroidb = np.average(cloudb, axis=1)\n",
        "\n",
        "    clouda_prime = clouda - centroida[:, np.newaxis]\n",
        "    cloudb_prime = cloudb - centroidb[:, np.newaxis]\n",
        "    H = np.sum(clouda_prime.T[:,:,None]*cloudb_prime.T[:,None], axis=0)\n",
        "\n",
        "    aRb = gtsam.Rot3.ClosestTo(H)\n",
        "    rot_centroidb = aRb.rotate(gtsam.Point3(centroidb))\n",
        "    atb = gtsam.Point3(centroida - np.array([rot_centroidb[0], rot_centroidb[1], rot_centroidb[2]]))\n",
        "\n",
        "    return gtsam.Pose3(aRb, atb).inverse()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hwrphcs80J4n"
      },
      "source": [
        "suite = unittest.TestSuite()\n",
        "suite.addTest(TestICPHelpers('test_estimate_transform1'))\n",
        "suite.addTest(TestICPHelpers('test_estimate_transform2'))\n",
        "unittest.TextTestRunner().run(suite)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A__i0XTTaXVH"
      },
      "source": [
        "## ICP Implementation\n",
        "\n",
        "Step 5: Repeat steps 2-4 for maximum number of iterations or change is very small.\n",
        "\n",
        "Per each iteration of ICP, give transformed clouda and cloudb as inputs to `assign_closest_pairs()`.\n",
        "\n",
        "The first output, `transform`, is a gtsam.Pose3 object, which is a transform describing how to transform clouda to align with cloudb (see Step 4 for explanation of this type of transform). The second output (optional), `icp_series`, lets you visualize each iteration of your ICP algorithm. The format of `icp_series` is an list of lists.\n",
        "\n",
        "Example: `[[clouda_iter1, cloudb_iter1], [clouda_iter2, cloudb_iter2], [clouda_iter3, cloudb_iter3], ...]`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9H2rtP5fr4Ab"
      },
      "source": [
        "def icp(clouda, cloudb, initial_transform=gtsam.Pose3(), max_iterations=25):\n",
        "    \"\"\"Runs ICP on two clouds by calling\n",
        "    all five steps implemented above.\n",
        "    Iterates until close enough or max\n",
        "    iterations.\n",
        "    \"\"\"\n",
        "    transform = gtsam.Pose3()\n",
        "    transform = initial_transform\n",
        "    icp_series = [[clouda, cloudb]]\n",
        "\n",
        "    for _ in range(max_iterations):\n",
        "      transformCloud = transform_cloud(transform, clouda)\n",
        "      closestPointPairs = assign_closest_pairs(transformCloud, cloudb)\n",
        "      estimated = estimate_transform(clouda, closestPointPairs)\n",
        "      icp_series.append([transformCloud, closestPointPairs])\n",
        "      if transform.range(estimated) < 0.1:\n",
        "        break\n",
        "      transform = estimated\n",
        "\n",
        "    return transform, icp_series"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tDDYp-x00J4o"
      },
      "source": [
        "suite = unittest.TestSuite()\n",
        "suite.addTest(TestICPHelpers('test_icp1'))\n",
        "suite.addTest(TestICPHelpers('test_icp2'))\n",
        "suite.addTest(TestICPHelpers('test_icp3'))\n",
        "unittest.TextTestRunner().run(suite)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "au61in0x3tNQ"
      },
      "source": [
        "Below is a simple test for ICP before running it on bigger point clouds. Each point cloud contains 3 points as corners of a triangle."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JLuw1LUi4TBY",
        "scrolled": false
      },
      "source": [
        "# create 2 triangles\n",
        "triangle = np.array([[0.0, 1.0, 1.0],\n",
        "                 [0.0, 0.0, 0.5],\n",
        "                 [0.0, 0.0, 0.0]])\n",
        "triangle_translated = triangle + 1.0\n",
        "\n",
        "# Use ICP to find the transform between them\n",
        "transform, icp_series = icp(triangle_translated, triangle)\n",
        "print(transform_cloud(transform, triangle_translated))\n",
        "print(triangle)\n",
        "print(transform)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4TSSQoD1dr1-"
      },
      "source": [
        "Now let's run ICP on the LIDAR data.  \n",
        "The animation shows how clouda has moved after each iteration of ICP. You should see stationary landmarks, like walls and parked cars, converge onto each other."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uQQ18QxLccZF"
      },
      "source": [
        "transform, icp_series = icp(clouda, cloudb)\n",
        "print(transform)\n",
        "\n",
        "visualize_clouds([transform_cloud(transform, clouda), cloudb], show_grid_lines=True, cloud_colors=['#008B8B', '#FFA07A'])\n",
        "# If you have returned a series of intermediate ICP clouds to visualize, you can uncomment the line below\n",
        "# visualize_clouds_animation(icp_series, speed=400, show_grid_lines=True, cloud_colors=['#008B8B', '#FFA07A'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VEqFxjzlpeH7"
      },
      "source": [
        "# GTSAM Introduction\n",
        "\n",
        "This section provides a brief introduction to the GTSAM library, which will be used to create a factor graph for pose optimization in the next section.\n",
        "\n",
        "First, let's get faimiliar with represeting and visualizing 3D pose in GTSAM by running the following code block."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mrVuyd0tpzH7"
      },
      "source": [
        "# translation is represented by a Point3 object (same as numpy array with \n",
        "# lenghth 3). The 3 elements corresponds to x, y, z values\n",
        "translation = gtsam.Point3(1, 1, 2)\n",
        "\n",
        "# rotation is represented by Rot3 class. There are several ways to create a Rot3. \n",
        "# We create one by specifying the yaw, pitch, roll angles\n",
        "rotation = gtsam.Rot3.Ypr(np.pi/2, 0, 0)\n",
        "\n",
        "# 3D pose is represeted by Pose3 class, which can be constructed with its \n",
        "# rotational (Rot3) and translational (Point3) componenets\n",
        "pose = gtsam.Pose3(rotation, translation)\n",
        "\n",
        "# you can directly use print to show the pose. The rotation is shown as a 3x3 \n",
        "# rotation matrix, and its translation is shown as a 3-vector\n",
        "print('pose:\\n', pose)\n",
        "\n",
        "# The uncertainty of the pose is reprsented its covariance matrix, which is of \n",
        "# shape 6x6. The upper-left 3x3 submatrix represents the covariance for rotation, \n",
        "# and the lower-right 3x3 submatrix represents the covariance for translation.\n",
        "marginal = np.identity(6)\n",
        "marginal[3, 3] = 5\n",
        "marginal[4, 4] = 1\n",
        "marginal[5, 5] = 0.5\n",
        "\n",
        "# You can use the plot_pose3 function to visualize the pose with uncertaintites\n",
        "# The x, y, z axes are represented with red, green, blue.\n",
        "fig_num = 0\n",
        "gtsam_plot.plot_pose3(fig_num, pose, axis_length=40, P=marginal);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mqg7cOp5vb-j"
      },
      "source": [
        "The following code block introduces factor graph optimization with a simple pose graph example. The factor graph includes 5 variables representing the poses of the robot at 5 time steps. There's a prior factor on the first variable, \"between factor\" (represeting odometry measurement) connecting consecutive poses, and a loop closure factor."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g6UW4guRAC62"
      },
      "source": [
        "# # Factor graph example \n",
        "\n",
        "################################################################################\n",
        "########################## Create noise model ##################################\n",
        "################################################################################\n",
        "# noise model represents the uncertainty for the measurements. In this example, \n",
        "# we directly specify uncertainty with standard deviation (sigma). The first 3 \n",
        "# elements represent the sigmas of rotation; the last 3 elements represent the \n",
        "# sigmas of translation\n",
        "prior_sigmas = np.array([0.1, 0.1, 0.1, 0.3, 0.3, 0.3])/20\n",
        "between_sigmas = np.array([0.1, 0.1, 0.1, 0.2, 0.2, 0.2])/20\n",
        "prior_noise = gtsam.noiseModel.Diagonal.Sigmas(prior_sigmas)\n",
        "between_noise = gtsam.noiseModel.Diagonal.Sigmas(between_sigmas)\n",
        "\n",
        "################################################################################\n",
        "######################## Create the factor graph ###############################\n",
        "################################################################################\n",
        "# - As you've learned that a factor graph consists of variable nodes and factor\n",
        "# nodes. In GTSAM, a factor graph is a container of factors, while we do not \n",
        "# explicitly store variables in a factor graph. \n",
        "# - A variable is referred to by a unique \"name\" (an integer). We use 1, 2, 3,\n",
        "# 4, 5 for the \"names\" of the 5 poses.\n",
        "# - In this example, we will create a factor graph as follows:\n",
        "#           5---•---4\n",
        "#           |       |\n",
        "#           •       •\n",
        "#           |       |\n",
        "#  •--1--•--2---•---3\n",
        "\n",
        "# create an empty factor graph. \n",
        "example_graph = gtsam.NonlinearFactorGraph()\n",
        "\n",
        "# - We first create a prior factor  on the first pose (with name 1), with prior \n",
        "# value at the origin, and add it to the factor graph\n",
        "prior_factor = gtsam.PriorFactorPose3(1, gtsam.Pose3(), prior_noise)\n",
        "example_graph.add(prior_factor)\n",
        "\n",
        "# We then add \"between factors\" that represent odometry measurements to connect \n",
        "# consecutive pose variables. A between factor is specified by the name of the \n",
        "# first pose variable, the name of the second pose variable, and the relative \n",
        "# transform between the two poses.\n",
        "# Notice that the noise model is required for all factors\n",
        "T_12 = gtsam.Pose3(gtsam.Rot3.Rz(0), gtsam.Point3(2, 0, 0))\n",
        "T_23 = gtsam.Pose3(gtsam.Rot3.Rz(math.pi/2), gtsam.Point3(2, 0, 0))\n",
        "T_34 = gtsam.Pose3(gtsam.Rot3.Rz(math.pi/2), gtsam.Point3(2, 0, 0))\n",
        "T_45 = gtsam.Pose3(gtsam.Rot3.Rz(math.pi/2), gtsam.Point3(2, 0, 0))\n",
        "example_graph.add(gtsam.BetweenFactorPose3(1, 2, T_12, between_noise)) \n",
        "example_graph.add(gtsam.BetweenFactorPose3(2, 3, T_23, between_noise)) \n",
        "example_graph.add(gtsam.BetweenFactorPose3(3, 4, T_34, between_noise)) \n",
        "example_graph.add(gtsam.BetweenFactorPose3(4, 5, T_45, between_noise)) \n",
        "\n",
        "# The loop closure factor is also represented as \"between factor\"\n",
        "T_52 = gtsam.Pose3(gtsam.Rot3.Rz(math.pi/2), gtsam.Point3(2, 0, 0))\n",
        "example_graph.add(gtsam.BetweenFactorPose3(5, 2, T_52, between_noise)) \n",
        "\n",
        "\n",
        "################################################################################\n",
        "######################## Create initial guess ##################################\n",
        "################################################################################\n",
        "# - Initial values are necessary for nonlinear optimization\n",
        "# - GTSAM uses \"Values\" type to store the values for variables. \"Values\" is like\n",
        "# a dictionary object in python, it maps the unique \"name\" of the variable to \n",
        "# its value\n",
        "# - The initial values do not need to be very precise.\n",
        "\n",
        "# create empty Values object\n",
        "example_initial_estimate = gtsam.Values()\n",
        "\n",
        "# add initial estiamtes of poses to Values\n",
        "T_w1 = gtsam.Pose3(gtsam.Rot3.Rz(0.2), gtsam.Point3(0.5, 0.0, 0.0))\n",
        "T_w2 = gtsam.Pose3(gtsam.Rot3.Rz(-0.2), gtsam.Point3(2.3, 0.0, 0.0))\n",
        "T_w3 = gtsam.Pose3(gtsam.Rot3.Rz(math.pi/2), gtsam.Point3(4.1, 0.3, 0.0))\n",
        "T_w4 = gtsam.Pose3(gtsam.Rot3.Rz(math.pi), gtsam.Point3(3.9, 2.2, 0.0))\n",
        "T_w5 = gtsam.Pose3(gtsam.Rot3.Rz(-math.pi), gtsam.Point3(2.1, 2.1, 0.0))\n",
        "example_initial_estimate.insert(1, T_w1)\n",
        "example_initial_estimate.insert(2, T_w2)\n",
        "example_initial_estimate.insert(3, T_w3)\n",
        "example_initial_estimate.insert(4, T_w4)\n",
        "example_initial_estimate.insert(5, T_w5)\n",
        "\n",
        "################################################################################\n",
        "################################# optimize #####################################\n",
        "################################################################################\n",
        "# - Optimize the initial values using a Gauss-Newton nonlinear optimizer\n",
        "\n",
        "# set parameters for the optimizer\n",
        "opt_parameters = gtsam.GaussNewtonParams()\n",
        "opt_parameters.setRelativeErrorTol(1e-5)\n",
        "opt_parameters.setMaxIterations(100)\n",
        "\n",
        "# create the optimization problem with the factor graph and initial values\n",
        "ex_optimizer = gtsam.GaussNewtonOptimizer(example_graph, example_initial_estimate, opt_parameters)\n",
        "\n",
        "# solving the optimization problem returns the optimized values for variables\n",
        "ex_result = ex_optimizer.optimize()\n",
        "\n",
        "################################################################################\n",
        "############################### visualization ##################################\n",
        "################################################################################\n",
        "# you can use print to show factor graphs and values\n",
        "print(\"Factor Graph:\\n\", example_graph)\n",
        "print(\"Final Result:\\n\", ex_result)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CXWQaUp8EO-c"
      },
      "source": [
        "# the uncertainties for each variable can be extracted by the \"marginal\" \n",
        "# probablity density of the variable\n",
        "marginals = gtsam.Marginals(example_graph, ex_result)\n",
        "\n",
        "# Visualize the optimzied poses with uncertainties\n",
        "fig = plt.figure(0, figsize=(8, 8), dpi=120)\n",
        "ax = fig.gca(projection='3d')\n",
        "for i in range(1, 6):\n",
        "    gtsam_plot.plot_pose3(0, ex_result.atPose3(i), 0.5, marginals.marginalCovariance(i))\n",
        "\n",
        "def axisEqual3D(ax):\n",
        "    extents = np.array([getattr(ax, 'get_{}lim'.format(dim))() for dim in 'xyz'])\n",
        "    sz = extents[:,1] - extents[:,0]\n",
        "    centers = np.mean(extents, axis=1)\n",
        "    maxsize = max(abs(sz))\n",
        "    r = maxsize/2\n",
        "    for ctr, dim in zip(centers, 'xyz'):\n",
        "        getattr(ax, 'set_{}lim'.format(dim))(ctr - r, ctr + r)\n",
        "\n",
        "axisEqual3D(ax)\n",
        "ax.view_init(azim=-90, elev=90)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O1IqCZ1ZvGzt"
      },
      "source": [
        "# Pose Graph Optimization\n",
        "\n",
        "In this section, we'll build a factor graph to estimate the pose of our vehicle using the transforms our ICP algorithm gives us between frames. These ICP transforms are the factors that tie the pose variables together."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MCswJeHPxY1S"
      },
      "source": [
        "We will be using GTSAM to construct the factor graph as well as perform a optimization for the pose of the car as it travels down the street. Recall from PoseSLAM describe in the LIDAR slides how we could add a factor (aka constraint) between two state variables. When we revisited a state, we could add a loop closure. Since the car in our dataset never revisits a previous pose, there is no loop closure."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ce1LpA3_rInm"
      },
      "source": [
        "import unittest\n",
        "\n",
        "class TestFactorGraph(unittest.TestCase):\n",
        "\n",
        "    def setUp(self):\n",
        "        max_steps = 6\n",
        "        test_clouds = read_ply(*scans_fnames)[:max_steps]\n",
        "        PRIOR_NOISE = gtsam.noiseModel.Diagonal.Sigmas(np.array([1e-6, 1e-6, 1e-6, 1e-4, 1e-4, 1e-4]))\n",
        "        ICP_NOISE = gtsam.noiseModel.Diagonal.Sigmas(np.array([1e-6, 1e-6, 1e-6, 1e-4, 1e-4, 1e-4]))\n",
        "\n",
        "        initial_pose = gtsam.Pose3(gtsam.Rot3(0.9982740, -0.0572837,  0.0129474, 0.0575611,  0.9980955, -0.0221840, -0.0116519,  0.0228910,  0.9996701),\n",
        "                          gtsam.Point3(-263.9464864482589, 2467.3015467381383, -19.374652610889633))\n",
        "        test_graph, test_initial_estimates = construct_graph_and_initial_estimates(initial_pose, test_clouds, PRIOR_NOISE, ICP_NOISE)\n",
        "        self.max_steps = max_steps\n",
        "        self.graph = test_graph\n",
        "        self.initial_estimates = test_initial_estimates\n",
        "    \n",
        "    def test_graph_params(self):\n",
        "        self.assertTrue(type(self.graph) == gtsam.NonlinearFactorGraph)\n",
        "        # 1 prior factor, 5 between factors connecting consecutive poses, 2 between\n",
        "        # factors for skip connection\n",
        "        self.assertTrue(self.graph.size() == self.max_steps + int((self.max_steps-1)/2))\n",
        "    \n",
        "    def test_initial_estimates_params(self):\n",
        "        self.assertTrue(type(self.initial_estimates) == gtsam.Values)\n",
        "        self.assertTrue(self.initial_estimates.size() == self.max_steps)\n",
        "        for i in range(self.max_steps):\n",
        "            self.assertTrue(self.initial_estimates.exists(i))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8c_QZE3hSWue"
      },
      "source": [
        "\n",
        "The ICP implementation is used here to find the transform between two subsequent clouds. These transforms become the factors between pose variables in the graph. So, it needs to go through all the point clouds and run icp pair-wise to find the relative movement of the car. With these transformation, create a factor representing the transform between the pose variables.\n",
        "\n",
        "We talked about how loop closure helps us consolidate conflicting data into a better global estimate. Unfortunately, our car does not perform a loop closure. So, our graph would just be a long series of poses connected by icp-returned transforms. However, our lidar scans are noisy, which means that our icp-returned transforms are not perfect either. This ultimately results in incorrect vehicle poses and overall map. One way that we can augment our graph is through \"skip connections\". We simply run ICP between every other cloud, and add these skip connections into the graph. You can basically perform ICP between two non-consecutive point clouds and add that transform as a factor in the factor graph."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yus6fnRtSbod"
      },
      "source": [
        "def construct_graph_and_initial_estimates(initial_pose, clouds, prior_noise, icp_noise):\n",
        "    \"\"\"construct a factor graph and initial value estimates using transforms \n",
        "    generated from aligning point clouds with icp.\n",
        "\n",
        "    Returns a graph (gtsam.NonlinearFactorGraph) with prior factor on the first \n",
        "    pose, and the between factors between pairs of clouds, along with our input \n",
        "    initial_estimates (gtsam.Values) of initial estimates for our vehicle pose \n",
        "    in world coordinates.\n",
        "    \"\"\"\n",
        "    graph = gtsam.NonlinearFactorGraph()\n",
        "    initial_estimates = gtsam.Values()\n",
        "\n",
        "    graph.add(gtsam.PriorFactorPose3(0, initial_pose, prior_noise))\n",
        "    initial_estimates.insert(0, initial_pose)\n",
        "    factor_pose = initial_pose\n",
        "    previousTransform = gtsam.Pose3()\n",
        "\n",
        "    # Add ICP Factors between each pair of clouds\n",
        "    for i in trange(len(clouds) - 1):\n",
        "        T, _ = icp(clouds[i+1], clouds[i], initial_transform=previousTransform)\n",
        "        previousTransform = T\n",
        "        T = T.inverse()\n",
        "        graph.add(gtsam.BetweenFactorPose3(i, i+1, T, icp_noise))\n",
        "        factor_pose = factor_pose.compose(T)\n",
        "        initial_estimates.insert(i+1, factor_pose)\n",
        "\n",
        "    previousTransform = gtsam.Pose3()\n",
        "    # Add skip connections between every other frame\n",
        "    for i in trange(0, len(clouds) - 2, 2):\n",
        "        T, _ = icp(clouds[i+2], clouds[i], initial_transform=previousTransform)\n",
        "        previousTransform = T\n",
        "        T = T.inverse()\n",
        "        graph.add(gtsam.BetweenFactorPose3(i, i+2, T, icp_noise))\n",
        "\n",
        "    return graph, initial_estimates"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZQn_nFFQ0J4r"
      },
      "source": [
        "suite = unittest.TestSuite()\n",
        "suite.addTest(TestFactorGraph('test_graph_params'))\n",
        "suite.addTest(TestFactorGraph('test_initial_estimates_params'))\n",
        "unittest.TextTestRunner().run(suite)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G3Zt28AiSl9S"
      },
      "source": [
        "The real power of GTSAM will show here. In five lines, we'll setup a Gauss Newton nonlinear optimizer and optimize for the vehicle's poses in world coordinates.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LPsfUC33Si7p"
      },
      "source": [
        "# load in all clouds in our dataset\n",
        "clouds = read_ply(*scans_fnames)\n",
        "\n",
        "# We get the initial pose of the car from Argo AI's dataset, and we add it to the graph as such\n",
        "PRIOR_NOISE = gtsam.noiseModel.Diagonal.Sigmas(np.array([1e-6, 1e-6, 1e-6, 1e-4, 1e-4, 1e-4]))\n",
        "ICP_NOISE = gtsam.noiseModel.Diagonal.Sigmas(np.array([1e-6, 1e-6, 1e-6, 1e-4, 1e-4, 1e-4]))\n",
        "initial_pose = gtsam.Pose3(gtsam.Rot3(0.9982740, -0.0572837,  0.0129474, 0.0575611,  0.9980955, -0.0221840, -0.0116519,  0.0228910,  0.9996701),\n",
        "                           gtsam.Point3(-263.9464864482589, 2467.3015467381383, -19.374652610889633))\n",
        "\n",
        "# We'll use the function to create the factor graph and initial estiamtes\n",
        "graph, initial_estimates = construct_graph_and_initial_estimates(initial_pose, clouds, PRIOR_NOISE, ICP_NOISE)\n",
        "\n",
        "# Now optimize for the states\n",
        "parameters = gtsam.GaussNewtonParams()\n",
        "parameters.setRelativeErrorTol(1e-5)\n",
        "parameters.setMaxIterations(100)\n",
        "optimizer = gtsam.GaussNewtonOptimizer(graph, initial_estimates, parameters)\n",
        "result = optimizer.optimize()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XFRK6Rpi30Eh"
      },
      "source": [
        "Let's plot these poses to see how our vehicle moves."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G1C0nQbkBJvR",
        "scrolled": false
      },
      "source": [
        "poses_cloud = np.array([[], [], []])\n",
        "for i in range(len(clouds)):\n",
        "    poses_cloud = np.hstack([poses_cloud, np.array([[result.atPose3(i).x()], [result.atPose3(i).y()], [result.atPose3(i).z()]])])\n",
        "\n",
        "init_car_pose = gtsam.Pose3(gtsam.Rot3(0.9982740, -0.0572837,  0.0129474, 0.0575611,  0.9980955, -0.0221840, -0.0116519,  0.0228910,  0.9996701),\n",
        "                            gtsam.Point3(-263.9464864482589, 2467.3015467381383, -19.374652610889633))\n",
        "visualize_clouds([poses_cloud, transform_cloud(init_car_pose, clouds[0])], show_grid_lines=True, cloud_colors=['#ffa500', '#FFFFFF'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nqsRcwJFb-DC"
      },
      "source": [
        "# Mapping\n",
        "\n",
        "In this section, we'll tackle the mapping component of SLAM (Simulataneous Localization and Mapping). The previous section used a factor graph to localize our vehicle's poses in world coordinates. We'll now use those poses to form a map of the street from the point clouds.\n",
        "\n",
        "Given the poses and the clouds, this task is easy. We'll use the `transform_cloud` method from the ICP section to transform every other cloud in our dataset to be centered at the corresponding pose where the cloud was captured. Visualizing all of these clouds yields the complete map. We don't use every cloud in our dataset to reduce the amount of noise in our map while retaining plenty of detail."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F7y4isMh0J4s"
      },
      "source": [
        "Generate the full map using the set of clouds provided and the results of the optimization. Then plot these transformed clouds to visualize the entire map the car saw in 18 seconds."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vt8LidY8cARN"
      },
      "source": [
        "def transform_map(transforms, clouds):\n",
        "    \"\"\"Use the `transform_cloud` method to transform each\n",
        "    cloud in `clouds` by its pose given in `transforms`.\n",
        "    \"\"\"\n",
        "\n",
        "    clouds_w = []\n",
        "    for i in range(len(clouds)):\n",
        "        cloud_i = clouds[i]\n",
        "        cloud_w = transform_cloud(transforms.atPose3(i).inverse(), cloud_i)\n",
        "        clouds_w.append(cloud_w)\n",
        "    return clouds_w"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u1MV9h9s0J4t"
      },
      "source": [
        "visualize_clouds(transform_map(result, clouds), show_grid_lines=True, cloud_colors=\"#C6C6C6\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}